---

layout: highlight_page # Do not change this portion

title: Attention Actor-Critic algorithm for Multi-Agent Constrained Co-operative Reinforcement Learning


speaker: Raghuram Bharadwaj Diddigi

img: assets/img/highlights/2020/raghurambharadwajdiddigi2020.png

year: 2020

category: phd #should have either mtech or phd

report_video: K3_CjAMZ0RM

---

**ABSTRACT**

In this work, we consider the problem of computing optimal actions
for Reinforcement Learning (RL) agents in a co-operative setting,
where the objective is to optimize a common goal. However, in
many real-life applications, the agents are also required to satisfy
certain constraints specified on their actions. Under this setting, the
objective of the agents is to not only learn the actions that optimize
the common objective but also meet the specified constraints. In
recent times, the Actor-Critic algorithm with an attention mechanism has been successfully applied to obtain optimal actions for RL
agents in multi-agent environments. In this work, we extend this
algorithm to the constrained multi-agent RL setting.
KEYWORDS
Multi-agent Reinforcement Learning; Attention Mechanism
ACM Reference Format:
P. Parnikaâˆ—
, Raghuram Bharadwaj Diddigiâˆ—
, Sai Koti Reddy Dandaâˆ—
, and Shalabh Bhatnagar. 2021. Attention Actor-Critic algorithm for Multi-Agent Constrained Co-operative Reinforcement Learning: Extended Abstract. In Proc.
of the 20th International Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2021), Online, May 3â€“7, 2021, IFAAMAS, 3 pages 

**INTRODUCTION**

In a multi-agent constrained co-operative RL setting, the agents
have to learn actions that not only minimize the expected total
discounted cost but also respect the constraints specified. One approach to satisfy the constraints is to construct a modified cost as
a linear combination of the original cost and the constraint costs.
However, the weights to be associated with the costs are not known
upfront and need to be learned in a trial-and-error fashion. We
alleviate this problem by considering the Lagrangian formulation
of the problem and training dual Lagrange parameters that act
as weights for the constraint costs. In this work, we propose an
Actor-Critic algorithm for computing the optimal actions for agents
that makes use of the attention mechanism. A brief overview of
the comparison of our work with other works in the literature is
provided in Table 1.
âˆ—Equal contribution by the first three authors. Full paper is available at [20].
Proc. of the 20th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2021), U. Endriss, A. NowÃ©, F. Dignum, A. Lomuscio (eds.), May 3â€“7, 2021, Online.
Â© 2021 International Foundation for Autonomous Agents and Multiagent Systems
(www.ifaamas.org). All rights reserved

eferences Features
[7, 11, 16,
18, 19]
Deep RL algorithms for multi-agent setting.
Attention mechanism not considered.
[13, 14, 17]
Deep RL algorithms with Attention for
multi-agent setting. Constrained setting not
considered.
[3â€“5]
RL algorithms for single-agent constrained
setting. Multi-agent constrained setting not
considered.
[1, 15, 23]
Deep RL algorithms for single-agent
constrained setting. Multi-agent constrained
setting not considered.
[2, 6, 9, 10,
12, 21, 22,
24]
RL algorithms for multi-agent constrained
setting. Attention mechanism not considered.
Our Work Deep RL algorithm with Attention mechanism
for multi-agent Constrained setting.
Table 1: Comparison with other works in the Literature
2 MODEL
We now briefly discuss the constrained co-operative multi-agent
setting [8] described by the tuple < ğ‘›, ğ‘†, ğ´,ğ‘‡ , ğ‘˜, ğ‘1, . . . , ğ‘ğ‘š,ğ›¾ >. Here,
ğ‘› denotes the number of agents in the environment, ğ‘† = ğ‘†1 Ã— ğ‘†2 Ã—
Â· Â· Â· ğ‘†ğ‘› is the joint state space, ğ´ = ğ´1 Ã— Â· Â· Â· Ã— ğ´ğ‘› denotes the joint
action space and ğ‘‡ is the probability transition matrix. Single-stage
cost function (ğ‘˜) where ğ‘˜ (ğ‘ , ğ‘) denotes the cost incurred when joint
action ğ‘ is taken in state ğ‘ . Moreover, ğ‘1, . . . , ğ‘ğ‘š denote the singlestage cost functions for the constraints. Note that both the main
cost function (ğ‘˜) and constraint costs (ğ‘1, . . . , ğ‘ğ‘š) depend on the
joint action of the agents. Finally, ğ›¾ denotes the discount factor.
Let ğœ‹ğ‘–
: ğ‘†ğ‘– âˆ’â†’ Î”(ğ´ğ‘–) denote the policy of agent ğ‘–, where for a
given state of agent ğ‘–, ğœ‹ğ‘–(ğ‘ ğ‘–) is a probability distribution over its
actions. We now define the total discounted cost (ğ½) for a joint
policy ğœ‹ = (ğœ‹1, . . . , ğœ‹ğ‘›) as follows:
ğ½ (ğœ‹) = ğ¸
hÃ•ğœ
ğ‘¡=0
ğ›¾
ğ‘¡
ğ‘˜ (ğ‘ ğ‘¡
, ğœ‹ (ğ‘ ğ‘¡))i
, (1)
where ğ¸[.] is the expectation over the entire trajectory of states
with initial state ğ‘ 0 âˆ¼ ğ‘‘0, where ğ‘‘0 is a given probability distributionover states, ğœ is a finite stopping time and ğ‘ ğ‘¡
is the joint state at
time ğ‘¡. The ğ‘š constraints on the system are defined as follows:
ğ¸
hÃ•ğœ
ğ‘¡=0
ğ›¾
ğ‘¡
ğ‘ğ‘— (ğ‘ ğ‘¡
, ğœ‹ (ğ‘ ğ‘¡))i
â‰¤ ğ›¼ğ‘—
, âˆ€ğ‘— âˆˆ 1, . . . ,ğ‘š, (2)
where ğ›¼1, . . . , ğ›¼ğ‘š are pre-specified thresholds.
The objective of the agents in the multi-agent constrained cooperative RL setting is to compute a joint policy ğœ‹
âˆ— = (ğœ‹
âˆ—
1
, . . . , ğœ‹âˆ—
ğ‘›
)
that is the solution to the optimization problem
min
ğœ‹ âˆˆÎ 
ğ½ (ğœ‹) = ğ¸
hÃ•ğœ
ğ‘¡=0
ğ›¾
ğ‘¡
ğ‘˜ (ğ‘ ğ‘¡
, ğœ‹ (ğ‘ ğ‘¡))i
(3)
s.t ğ¸
hÃ•ğœ
ğ‘¡=0
ğ›¾
ğ‘¡
ğ‘ğ‘— (ğ‘ ğ‘¡
, ğœ‹ (ğ‘ ğ‘¡))i
â‰¤ ğ›¼ğ‘—
, âˆ€ğ‘— âˆˆ 1, . . . ,ğ‘š,
where Î  is the set of all joint policies. We make use of Lagrangian
approach to solve this constrained problem. The pseudo-code of
our proposed algorithm is described in Algorithm 1.
Algorithm 1 Multi-Agent Constrained Attention Actor-Critic
(MACAAC)
1: ğ¸ â†âˆ’ Maximum number of episodes.
2: ğ¿ â†âˆ’ Length of an episode.
3: ğ‘ˆ â†âˆ’ Steps per update.
4: ğœƒğ‘– â†âˆ’ policy parameters of the agent ğ‘–, ğ‘– = 1, . . . , ğ‘›.
5: UpdateCritic: Subroutine to update the critic parameters.
6: UpdateActors: Subroutine to update the policy parameters of
all the agents.
7: ğ‘„ğœ‚ğ‘— â†âˆ’ Q-value of constrained cost associated with constraint
ğ‘—, ğ‘— = 1, . . . ,ğ‘š.
8: ğ›½ğ‘¡ â†âˆ’ Slower timescale step-size at time step ğ‘¡.
9: Initialize Lagrange parameters ğœ†1, . . . , ğœ†ğ‘š.
10: Create ğœ‡ parallel environments.
11: Initialize replay buffer, D.
12: ğ‘¢ â†âˆ’ 0
13: for ğ‘’ğ‘ = 1, 2, . . . , ğ¸ do
14: Obtain initial observations ğ‘œ
ğ‘’
ğ‘–
for all agents ğ‘– in each
15: environment ğ‘’
16: for ğ‘¡ = 1, 2, . . . , ğ¿ do
17: Obtain actions ğ‘
ğ‘’
ğ‘–
âˆ¼ ğœ‹ğœƒğ‘–
(.|ğ‘œ
ğ‘’
ğ‘–
), âˆ€ğ‘– = 1, . . . , ğ‘›,
18: âˆ€ğ‘’ = 1, . . . , ğœ‡
19: Execute actions and get (ğ‘œ
âˆ—,ğ‘’
ğ‘–
, ğ‘˜ğ‘’
, ğ‘ğ‘’
1
, ğ‘ğ‘’
2
, . . . , ğ‘ğ‘’
ğ‘š) âˆ€ğ‘–, ğ‘’
20: Let ğ‘Ÿ
ğ‘’ = ğ‘˜
ğ‘’ +
Ã•ğ‘š
ğ‘—=1
ğœ†ğ‘—ğ‘
ğ‘’
ğ‘—
, âˆ€ğ‘’
21: Store (ğ‘œ
ğ‘’
ğ‘–
, ğ‘ğ‘’
ğ‘–
, ğ‘Ÿğ‘’
, ğ‘ğ‘’
1
, ğ‘ğ‘’
2
, . . . , ğ‘ğ‘’
ğ‘š, ğ‘œ
âˆ—,ğ‘’
ğ‘–
), âˆ€ğ‘–, ğ‘’ in ğ·
22: ğ‘œ
ğ‘’
ğ‘–
= ğ‘œ
âˆ—,ğ‘’
ğ‘–
, âˆ€ğ‘–, ğ‘’
23: ğ‘¢+ = ğœ‡
24: if (ğ‘¢% U) < ğœ‡ then
25: Sample minibatch (B) from D
26: Get next actions ğ‘
â€²
1
, . . . , ğ‘
â€²
ğ‘›
27: UpdateCritic(B, ğ‘
â€²
1
, . . . , ğ‘
â€²
ğ‘›
)
28: UpdateActors(B)
29: for ğ‘— = 1, . . . ,ğ‘š do
30: ğœ†ğ‘— â†âˆ’ max(0, ğœ†ğ‘— + ğ›½ğ‘¡ (ğ‘„ğœ‚ğ‘— âˆ’ ğ›¼ğ‘—))

3 EXPERIMENTS AND RESULTS

In the constrained version of Cooperative Navigation [16] that we
consider, there are 5 agents and 5 targets that are randomly generated in a continuous environment at the beginning of each episode.
The objective of the agents is to navigate towards the targets in a
co-operative manner such that all targets are covered. The length
of each episode is 25 time steps and the single-stage cost at each
time step is the sum of the distance to the nearest agent, over all the
targets. Therefore, the agents have to learn to navigate towards the
targets in such a way that all target positions are covered. However,
we include a single-stage penalty of 1 when there is a collision between the agents (and 0 otherwise). The penalty threshold (ğ›¼) is set
to 3 in our experiments. This means that the expected total penalty
over all the episodes must be less than or equal to 3. The discount
factor is set to 0.99. We refer to the main cost that the agents are
minimizing as â€˜costâ€™ and the constrained cost as the â€˜penaltyâ€™. For
comparison purposes, we also implement the constrained version
of MADDPG [16] algorithm, which we refer to as â€˜MADDPG-Câ€™.
Moreover, to better analyze the results, we also report the results
on an un-constrained version of Multi-agent Attention Actor-Critic
[13] where there is no penalty incurred for collisions among the
agents, which we simply refer to as â€˜Unconstrainedâ€™.

3.1 Discussion

In Figure 1a, we observe that the total cost approaches convergence
for all the three algorithms. The â€˜Unconstrainedâ€™ algorithm achieves
the smallest average cost as there is no penalty for collisions in
this case. Therefore, the agents can move freely in the continuous
space and navigate quickly towards the targets. This can also be
observed in Figure 1b, where we see that the average penalty of the
â€˜Unconstrainedâ€™ algorithm is the highest. In Figure 1b, we see that
the average penalty comes down as the training progresses for the
constrained algorithms (MADDPG-C and our proposed MACAAC),
while for the â€˜unconstrainedâ€™ algorithm it almost remains constant.
This is the effect of Lagrange parameters that are learnt in the
constrained setting.
(a) Expected total cost (b) Expected total penalty
Figure 1: Performance of Algorithms on Constrained Cooperative Navigation during the training.

4 ACKNOWLEDGEMENTS

Raghuram Bharadwaj Diddigi was supported by a fellowship grant
from the Centre for Networked Intelligence (a Cisco CSR initiative)
of the Indian Institute of Science, Bangalore. Shalabh Bhatnagar
was supported by the J.C.Bose Fellowship, the RBCCPS, IISc and a
research project from DST, India.
Extended Abstract AAMAS 2021, May 3-7, 2021, Online REFERENCES
[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. 2017. Constrained
policy optimization. arXiv preprint arXiv:1705.10528 (2017).
[2] Pritee Agrawal, Pradeep Varakantham, and William Yeoh. 2016. Scalable greedy
algorithms for task/resource constrained multi-agent stochastic planning. (2016).
[3] Shalabh Bhatnagar. 2010. An actorâ€“critic algorithm with function approximation
for discounted cost constrained Markov decision processes. Systems & Control
Letters 59, 12 (2010), 760â€“766.
[4] Shalabh Bhatnagar and K Lakshmanan. 2012. An online actorâ€“critic algorithm
with function approximation for constrained markov decision processes. Journal
of Optimization Theory and Applications 153, 3 (2012), 688â€“708.
[5] Vivek S Borkar. 2005. An actor-critic algorithm for constrained Markov decision
processes. Systems & control letters 54, 3 (2005), 207â€“213.
[6] Craig Boutilier and Tyler Lu. 2016. Budget allocation using weakly coupled,
constrained Markov decision processes. (2016).
[7] Gang Chen. 2019. A New Framework for Multi-Agent Reinforcement Learningâ€“
Centralized Training and Exploration with Decentralized Execution via Policy
Distillation. arXiv preprint arXiv:1910.09152 (2019).
[8] Raghuram Bharadwaj Diddigi, Sai Koti Reddy Danda, Prabuchandran K.J., and
Shalabh Bhatnagar. 2019. Actor-Critic Algorithms for Constrained Multi-agent
Reinforcement Learning. arXiv preprint arXiv:1905.02907 (2019).
[9] Raghuram Bharadwaj Diddigi, D Reddy, Prabuchandran KJ, and Shalabh Bhatnagar. 2019. Actor-Critic Algorithms for Constrained Multi-agent Reinforcement Learning. In Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems. International Foundation for Autonomous Agents
and Multiagent Systems, 1931â€“1933.
[10] Dmitri A Dolgov and Edmund H Durfee. 2011. Resource Allocation Among
Agents with MDP-Induced Preferences. arXiv preprint arXiv:1110.2767 (2011).
[11] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and
Shimon Whiteson. 2017. Counterfactual multi-agent policy gradients. arXiv
preprint arXiv:1705.08926 (2017).
[12] Michael Fowler, Pratap Tokekar, T Charles Clancy, and Ryan K Williams. 2018.
Constrained-Action POMDPs for Multi-Agent Intelligent Knowledge Distribution.
In 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE,
1â€“8.
[13] Shariq Iqbal and Fei Sha. 2019. Actor-Attention-Critic for Multi-Agent Reinforcement Learning. In Proceedings of the 36th International Conference on Machine
Learning (Proceedings of Machine Learning Research, Vol. 97). PMLR, Long Beach,
California, USA, 2961â€“2970. http://proceedings.mlr.press/v97/iqbal19a.html
[14] Jiechuan Jiang and Zongqing Lu. 2018. Learning attentional communication for
multi-agent cooperation. In Advances in Neural Information Processing Systems.
7254â€“7264.
[15] Qingkai Liang, Fanyu Que, and Eytan Modiano. 2018. Accelerated primal-dual policy optimization for safe reinforcement learning. arXiv preprint arXiv:1802.06480
(2018).
[16] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch.
2017. Multi-agent Actor-Critic for Mixed Cooperative-Competitive Environments.
In Advances in Neural Information Processing Systems. 6379â€“6390.
[17] Hangyu Mao, Zhengchao Zhang, Zhen Xiao, and Zhibo Gong. 2019. Modelling the
dynamic joint policy of teammates with attention multi-agent ddpg. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent
Systems. 1108â€“1116.
[18] Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. 2020. Deep reinforcement learning for multiagent systems: A review of challenges, solutions,
and applications. IEEE transactions on cybernetics (2020).
[19] Afshin OroojlooyJadid and Davood Hajinezhad. 2019. A review of cooperative
multi-agent deep reinforcement learning. arXiv preprint arXiv:1908.03963 (2019).
[20] P. Parnika, Raghuram Bharadwaj Diddigi, Sai Koti Reddy Danda, and Shalabh
Bhatnagar. 2021. Attention Actor-Critic algorithm for Multi-Agent Constrained
Co-operative Reinforcement Learning. arXiv e-print arXiv:2101.02349 (2021).
[21] D Sai Koti Reddy, Amrita Saha, Srikanth G Tamilselvam, Priyanka Agrawal, and
Pankaj Dayama. 2019. Risk averse reinforcement learning for mixed multi-agent
environments. In Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems. 2171â€“2173.
[22] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. 2016. Safe,
multi-agent, reinforcement learning for autonomous driving. arXiv preprint
arXiv:1610.03295 (2016).
[23] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. 2018. Reward constrained
policy optimization. arXiv preprint arXiv:1805.11074 (2018).
[24] Ruohan Zhang, Yue Yu, Mahmoud El Chamie, BehÃ§et AÃ§ikmese, and Dana H
Ballard. 2016. Decision-Making Policies for Heterogeneous Autonomous MultiAgent Systems with Safety Constraints.. In IJCAI. 546â€“553.
