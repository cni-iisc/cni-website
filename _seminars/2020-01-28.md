---
layout: seminar_page
category: ""
n: 19
title:  Stochastic optimization with compressed gradients
speaker: Prathamesh Mayekar
img: assets/img/seminars/2020/January-28-2020_Prathamesh Mayekar.png
date: 2020-01-28 17:00:00 
Venue:
recorded_video: oFKCP668rZY
speaker_bio: "Prathamesh Mayekar is a fourth year Ph.D. candidate in the Department of Electrical Communication Engineering at the Indian Institute of Science, Bengaluru. He is advised Dr. Himanshu Tyagi. He received his Master’s degree in Industrial Engineering and Operation Research from the Indian Institute of Technology Bombay in 2015 and a Bachelor’s degree in Electronics and Telecom. Engineering from the University of Mumbai in 2013. Broadly, his research interests lie at the intersection of information theory and optimization. He is a recipient of Jack Keil Wolf ISIT Student Paper Award and Wipro PhD fellowship."



---

Topic_abstract: "We consider stochastic optimization over $\ell_p$ spaces using access to a first-order oracle. In the first part of the talk, we ask: What is the minimum precision required for oracle outputs to retain the unrestricted convergence rates? We characterize this precision for every $p\geq 1$ by deriving information theoretic lower bounds and by providing quantizers that (almost) achieve these lower bounds. In the second part of the talk, we completely characterize the precision-convergence trade-off for the Euclidean case. Interestingly, the quantizer designed for this setting, RATQ, (almost) achieves the rate-distortion bounds universally for the well-studied Gaussian rate-distortion problem. This talked is based on joint work with Himanshu Tyagi."
