--- 

  

layout: seminar_page 

category: "" 

n: 185

  

title: "Provably Robust DPO: Aligning Language Models with Noisy Feedback" 

speaker: "Sayak Ray Chowdhury, postdoctoral researcher at Microsoft Research, India"  

img: assets/img/seminars/2024/05-03-2024 Sayak Ray Chowdhury.png

date: 2024-03-05 17:00:00  

Venue: MP 30 and Online on Zoom 

  

recorded_video: 0LKFGEhIDkA

zoom_link: https://us06web.zoom.us/j/83388976389?pwd=XcpO3GhLxsR14a7SVbPx33HQQa1jbt.1 

slides:  

speaker_url: https://sites.google.com/view/sayakraychowdhury/home?authuser=0

speaker_bio: "Sayak Ray Chowdhury is a postdoctoral researcher at Microsoft Research, India. Prior to this he was a postdoctoral fellow at Boston University, USA. He obtained his PhD from the Dept of ECE, IISc, where he was a recipient of Google PhD fellowship. His research interests include reinforcement learning, Bayesian optimization, multi-armed bandits and differential privacy. Recently, he has been working towards mathematical and empirical understandings of language models."

Topic_abstract: "Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. These aligned models demonstrate impressive capabilities across various tasks. However, noisy preference data can negatively impact alignment. Practitioners have recently proposed heuristics to mitigate the effect, but theoretical underpinnings of these methods have remained elusive. In this work, we aim to bridge this gap by introducing a general framework for policy optimization in the presence of random preference flips. We propose rDPO, a robust version of the popular direct preference optimization method, to show that it is provably tolerant to noise, and characterize its sub-optimality gap as a function of noise rate, dimension of the policy parameter, and sample size. Experiments on two real datasets show that rDPO is robust to noise in preferences compared to vanilla DPO and heuristics proposed by practitioners."
--- 
